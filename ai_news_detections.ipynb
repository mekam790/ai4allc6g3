{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing: AI Detection using Machine Learning Techniques\n",
    "\n",
    "**Group Members: Nathanielle, Rishika, Michelle, Ying Lin**\n",
    "\n",
    "**Research Question** : How can AI be used to detect if a political article is real or AI-generated? What keywords are significant in differentiating between a real and AI-generated article?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "insert stuff here - context of rise of AI and significance of analysis (if need inspo, look at ying's branch???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "\n",
    "Feel free to make edits - Ying\n",
    "For our statistical analysis, we will be using the GoodNews dataset, which was created by Ali Fukan Biten, Lluis Gomez, Marcal Rusiñol, and Dimosthenis Karatzas for their research on image captioning. The GoodNews dataset is composed of articles from New York Times from 2010 and 2018 (Biten, Gomez, Rusiñol, Karatzas 3).\n",
    "\n",
    "The widespread integration and use of generative AI tools such as ChatGPT and Microsoft Copilot posed the initial concern that human-generated text used these tools, which would lead to a confounding factor that we can't measure and analyze. Generative AI rose in popularity in approximately 2022, as marked by the release of ChatGPT in November 2022 and the sudden spike in market value since 2022 (Bloomberg 2023). It is unlikely that AI used used in the creation of New York Times articles because Generative AI was in experimental phase and New York Times authors likely did not have significant evidence that proved the efficiacy of Generative AI.\n",
    "\n",
    "https://www.bloomberg.com/company/press/generative-ai-to-become-a-1-3-trillion-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "from sklearn.svm import LinearSVC\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(\"news_dataset.csv\")\n",
    "print (f\"The shape of the original dataframe is {news_df.shape}\")\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converted all articles to lower case\n",
    "news_df['Articles'] = [article.lower()for article in news_df[\"Articles\"]]\n",
    "# Filter the articles to only political articles\n",
    "filtered_df = news_df[news_df[\"Articles\"].str.contains(\"\"\" election|\n",
    "             campaign| vote| ballot| voting| polling| candidate| nominee| politician|\n",
    "             leader| opposition| incumbent| poll| polling| approval rating|\n",
    "             electorate| conservative| liberal| democrat| republican| left-wing|\n",
    "             right-wing| centrist| far-right| far-left| populist|\n",
    "             governor| mayor| senator| representative| joe biden| bernie sanders| \n",
    "             elizabeth warren| pete buttigieg| andrew yang| tulsi gabbard| \n",
    "             kamala harris\"\"\", case=False)]\n",
    "\n",
    "# Clean the labels column into fake (AI generated) versus not fake (human generated)\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df.loc[filtered_df[\"Labels\"] == \"fake\", \"Labels\"] = 1\n",
    "filtered_df.loc[filtered_df[\"Labels\"] == \"real\", \"Labels\"] = 0\n",
    "filtered_df = filtered_df.rename(columns={'Labels': 'Fake', 'Articles':'Article'})\n",
    "filtered_df[\"Fake\"] = filtered_df[\"Fake\"].astype(int)\n",
    "print(f'The shape of the filtered data frame is: {filtered_df.shape}')\n",
    "print (f\"Number of real articles {filtered_df.shape[0] - sum(filtered_df['Fake'])}\")\n",
    "print (f\"Number of AI generated articles {sum(filtered_df['Fake'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_ai = pd.DataFrame(filtered_df[filtered_df['Fake']==1])\n",
    "unprocessed_real = pd.DataFrame(filtered_df[filtered_df['Fake']==0])\n",
    "fake_unprocessed_sample = unprocessed_ai.sample(5000, random_state= 2950)\n",
    "real_unprocessed_sample = unprocessed_real.sample(5000, random_state= 2950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(article):\n",
    "    \"\"\"\n",
    "    article(string): Text to be cleaned for text analysis\n",
    "    A function that accepts article and removes the punctuation, pronouns,\n",
    "    and commonly used words that don't provide additional information such as \n",
    "    'the', 'a', etc.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    article = ''.join([char for char in article if char not in string.punctuation])\n",
    "    # Lemmatization of all non-stop words\n",
    "    article = ' '.join([lemmatizer.lemmatize(word) for word in article.split() \n",
    "                        if word.isalpha() and word not in stop_words])\n",
    "    \n",
    "    return article\n",
    "\n",
    "filtered_df[\"Cleaned_Article\"] = filtered_df[\"Article\"].apply(text_preprocessing)\n",
    "filtered_df[\"Cleaned_Article\"] = filtered_df[\"Cleaned_Article\"].astype(\"string\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_articles = pd.DataFrame(filtered_df[filtered_df['Fake']==1])\n",
    "real_articles = pd.DataFrame(filtered_df[filtered_df['Fake']==0])\n",
    "\n",
    "fake_processed_sample = ai_articles.sample(n=5000, random_state= 2950)\n",
    "real_processed_sample = real_articles.sample(n=5000, random_state= 2950)\n",
    "merged_df = pd.concat([fake_processed_sample, real_processed_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_merged_df = pd.concat([real_processed_sample, fake_processed_sample])\n",
    "processed_merged_df.reset_index(inplace = True, drop = True)\n",
    "joint_1000text = \" \".join(article for article in real_processed_sample[\"Cleaned_Article\"].sample(1000, random_state= 2950))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "# add additional filtering to provide more insightful information\n",
    "stopwords.update([\"said\", \"would\", \"time\", \"year\", \"one\", \"city\", \"like\", \"mr\", \"new\", \"york\", \"times\"])\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                max_words= 200,\n",
    "                min_font_size = 10).generate(joint_1000text)\n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.title(\"Word Cloud for A Randomized Sample of 1000 Articles (Human Generated)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_1000text = \" \".join(article for article in fake_processed_sample[\"Cleaned_Article\"].sample(1000, random_state= 2950))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "# add additional filtering to provide more insightful information\n",
    "stopwords.update([\"said\", \"would\", \"time\", \"year\", \"one\", \"city\", \"like\", \"mr\", \"new\", \"york\", \"times\"])\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                max_words= 200,\n",
    "                min_font_size = 10).generate(joint_1000text)\n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.title(\"Word Cloud for A Randomized Sample of 1000 Articles (AI Generated)\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Analysis of Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_ngrams(text_series, ngram_range=(5, 5), top_n=5):\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english')\n",
    "    ngram_matrix = vectorizer.fit_transform(text_series)\n",
    "    ngram_counts = np.asarray(ngram_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "    ngram_terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    ngram_freq = Counter(dict(zip(ngram_terms, ngram_counts)))\n",
    "    top_ngrams = ngram_freq.most_common(top_n)\n",
    "\n",
    "    return top_ngrams\n",
    "\n",
    "# Separate the articles\n",
    "real_articles = filtered_df[filtered_df['Fake'] == 0]['Cleaned_Article']\n",
    "fake_articles = filtered_df[filtered_df['Fake'] == 1]['Cleaned_Article']\n",
    "\n",
    "# Retrieve the top 5-gram from  REAL articles and fake articles\n",
    "# Look into ngram range\n",
    "top_five_grams_real = get_top_ngrams(real_articles, ngram_range=(5, 5), top_n=5)\n",
    "top_five_grams_fake = get_top_ngrams(fake_articles, ngram_range=(5, 5), top_n=5)\n",
    "\n",
    "# Unzipping\n",
    "real_ngram_labels, real_ngram_values = zip(*top_five_grams_real)\n",
    "fake_ngram_labels, fake_ngram_values = zip(*top_five_grams_fake)\n",
    "\n",
    "#Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=list(real_ngram_values), y=list(real_ngram_labels), palette='Blues')\n",
    "plt.title('Top 5-grams in REAL Human-written Articles')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('5-gram')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=list(fake_ngram_values), y=list(fake_ngram_labels), palette='Reds')\n",
    "plt.title('Top 5-grams in FAKE AI-generated Articles')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('5-gram')\n",
    "\n",
    "# Graph Layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Length difference between AI generated text and human text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_unprocessed_sample[\"Word_Count\"] = fake_unprocessed_sample[\"Article\"].apply(lambda n: len(n.split()))\n",
    "real_unprocessed_sample[\"Word_Count\"] = real_unprocessed_sample[\"Article\"].apply(lambda n: len(n.split()))\n",
    "print (\"In the original data set: \")\n",
    "print(f\"\"\"On average, AI-Generated articles has approximately \n",
    "      {fake_unprocessed_sample['Word_Count'].mean():.2f} words per article.\"\"\")\n",
    "print(f\"\"\"On average, non AI-Generated articles has approximately \n",
    "      {real_unprocessed_sample['Word_Count'].mean():.2f} words per article.\"\"\")\n",
    "print (\"\\nAfter lemmatization and removing of stop words: \")\n",
    "fake_processed_sample[\"Word_Count\"] = fake_processed_sample[\"Cleaned_Article\"].apply(lambda n: len(n.split()))\n",
    "real_processed_sample[\"Word_Count\"] = real_processed_sample[\"Cleaned_Article\"].apply(lambda n: len(n.split()))\n",
    "print(f\"\"\"On average, AI-Generated articles has approximately \n",
    "      {fake_processed_sample['Word_Count'].mean():.2f} words per article.\"\"\")\n",
    "print(f\"\"\"On average, non AI-Generated articles has approximately \n",
    "      {real_processed_sample['Word_Count'].mean():.2f} words per article.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "#### Logistic Regression Analysis with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 10)\n",
    "X = vectorizer.fit_transform(merged_df[\"Cleaned_Article\"]).toarray()\n",
    "y = merged_df[\"Fake\"]  # 1 = fake, 0 = real\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2950)\n",
    "print (f\"\"\"The shape of the train sample is {X_train.shape} and the shape \n",
    "       for the test sample is {X_test.shape}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (n=100) Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='red', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_tree = rf_classifier.estimators_[0]\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "    individual_tree,\n",
    "    feature_names=vectorizer.get_feature_names_out(),\n",
    "    class_names=['Real', 'Fake'],\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render('individual_tree', view=True)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels for evaluation\n",
    "labels = processed_merged_df[\"Fake\"].tolist()\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "# Define the evaluation function\n",
    "def fit_and_evaluate(km, X, labels, name=None, n_runs=5):\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        km.fit(X)\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(metrics.adjusted_rand_score(labels, km.labels_))\n",
    "        scores[\"Silhouette Coefficient\"].append(metrics.silhouette_score(X, km.labels_, sample_size=2000))\n",
    "    \n",
    "    evaluation = {\"estimator\": name}\n",
    "    evaluation_std = {\"estimator\": name}\n",
    "    \n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    \n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homogeneity: Measures if each cluster contains only members of a single class. A low score (close to 0) indicates that the clusters are not homogeneous, meaning that the clusters contain a mix of different classes.\n",
    "\n",
    "Completeness: Measures if all members of a given class are assigned to the same cluster. A low score (close to 0) suggests that the members of each class are spread across multiple clusters, indicating poor clustering performance.\n",
    "\n",
    "V-measure: The harmonic mean of homogeneity and completeness. A low V-measure indicates that both homogeneity and completeness are low, reflecting poor overall clustering quality.\n",
    "\n",
    "Adjusted Rand-Index: Measures the similarity between the true labels and the clustering labels, adjusted for chance. A low score (close to 0) indicates that the clustering results are not much better than random assignment.\n",
    "\n",
    "Silhouette Coefficient: Measures how similar an object is to its own cluster compared to other clusters. A low score (close to 0) suggests that the clusters are not well-separated, and the data points are not well-clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "fit_and_evaluate(kmeans, X, labels=labels, name=\"KMeans on tf-idf vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, when it's set to two clusters and initialized using the greedy kmeans++ and the distribution of the points, it does not perform well. The purity of resulting clusters are low and the algorthm is not accurately clustering the article. To address this, possible steps could be to tune the parameters and use principal component analysis to reduce the amount of dimensions.\n",
    "\n",
    "First, I will experiment with the initialization of the centriods and setting it to random, where the inital centroids will be randomly selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, init = \"random\")\n",
    "fit_and_evaluate(kmeans, X, labels=labels, name=\"KMeans on tf-idf vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above values, we can see that randomly initializing the centroid did help with increasing the evaluation metrics. Using the random centriod initialization method, how would the changing the number of independent runs change this? When using a random centriod initialization method, the algorithm runs 10 times and the best output is returned. Would increasing the amount of times the algorithm is run enhance the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data exploration section, we see that there is an difference between article length and the source of the article (human generated or AI generated). Is this difference significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_merged_df[\"Article_Length\"] = processed_merged_df[\"Cleaned_Article\"].apply(lambda n: len(n.split()))\n",
    "Xtrain_len, Xtest_len, ytrain_len, ytest_len = train_test_split(\n",
    "    processed_merged_df[\"Article_Length\"].values.reshape(-1, 1), \n",
    "    processed_merged_df[\"Fake\"], \n",
    "    test_size=0.3, \n",
    "    random_state=2950\n",
    ")\n",
    "\n",
    "Xtrain_sm = sm.add_constant(Xtrain_len)\n",
    "logit_model = sm.Logit(ytrain_len, Xtrain_sm).fit()\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_sm = sm.add_constant(Xtest_len)\n",
    "ytest_len_prob = logit_model.predict(Xtest_sm)\n",
    "ytest_len_pred = [1 if prob > 0.65 else 0 for prob in ytest_len_prob]\n",
    "test_scores = classification_report(ytest_len, ytest_len_pred,zero_division=0)\n",
    "print (test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the classification report, we can observe that the model performs well in identifying AI-generated articles as shown by the high recall for AI-generated text but struggles significantly with human-generated articles as demonstrated by the scores. \n",
    "\n",
    "The overall accuracy is 48%, indicating that the model correctly predicts the class for 48% of the articles. This means that if one uses only length to infer if an article is AI-generated or not, it performs worst than randomly guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = pd.concat([pd.DataFrame(X), \n",
    "                        processed_merged_df[\"Article_Length\"].reset_index(drop=True)], axis=1)\n",
    "X_combined.columns = X_combined.columns.astype(str)\n",
    "fit_and_evaluate(kmeans, X_combined, labels=labels, name=\"KMeans on tf-idf vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although still low, we can see that adding article length significantly increased the silhouette coefficient, which reflect the defineness of clusters and did increase the other metrics such as homogeneity and completeness.\n",
    "\n",
    "From the EDA section, we see that the AI-generated article tend to use the phrase \"read the full story at\" while the human-generated article did not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4allc6g3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
