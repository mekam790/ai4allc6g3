{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages\n",
    "\n",
    "### Loading Data:\n",
    "Need to download actual dataset from https://drive.google.com/file/d/1vD4DtyJOIjRzchPtCQu-KPrUjgTiWSmo/view and unzip via Terminal (unzip NeuralNews.zip)\n",
    "\n",
    "### Creating new environment to avoid clashes\n",
    "python -m venv /Users/yzhao/ai4allc6g3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Filtering to election based articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/yzhao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake</td>\n",
       "      <td>A longtime champion of the homeless and batter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fake</td>\n",
       "      <td>Tucked away in the Marais, two warring groups ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fake</td>\n",
       "      <td>There are plenty of things that can impede wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fake</td>\n",
       "      <td>New York City is home to more than 2,500 tiny ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fake</td>\n",
       "      <td>A man wearing a hat emblazoned with the words ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Labels                                           Articles\n",
       "0   fake  A longtime champion of the homeless and batter...\n",
       "1   fake  Tucked away in the Marais, two warring groups ...\n",
       "2   fake  There are plenty of things that can impede wom...\n",
       "3   fake  New York City is home to more than 2,500 tiny ...\n",
       "4   fake  A man wearing a hat emblazoned with the words ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv(\"news_dataset.csv\")\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    A function that accepts string, text, and removes the punctuation, pronouns,\n",
    "    and commonly used words that don't provide additional information such as \n",
    "    'the', 'a', etc.\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "news_df['Cleaned_Article'] = news_df['Articles'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    longtime champion homeless battered times unio...\n",
       "1    tucked away marais two warring groups compete ...\n",
       "2    plenty things impede women’s road career succe...\n",
       "3    new york city home 2500 tiny churches yearroun...\n",
       "4    man wearing hat emblazoned words “driving libe...\n",
       "Name: Cleaned_Article, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['Cleaned_Article'].iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contains documentation: https://pandas.pydata.org/docs/reference/api/pandas.Series.str.contains.html\n",
    "# Filter to text that only includes the substring election, trump, biden, and harris\n",
    "filtered_df = news_df[news_df[\"Cleaned_Article\"].str.contains(\"election|trump|biden|harris\", case=False)]\n",
    "\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "filtered_df.loc[filtered_df[\"Labels\"] == \"fake\", \"Labels\"] = 1\n",
    "filtered_df.loc[filtered_df[\"Labels\"] == \"real\", \"Labels\"] = 0\n",
    "filtered_df = filtered_df.rename(columns={'Labels': 'Fake', 'Articles':'Article'})\n",
    "filtered_df[\"Fake\"] = filtered_df[\"Fake\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the filtered data frame is: (11559, 3)\n",
      "Number of real articles 4636\n",
      "Number of AI generated articles 6923\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fake</th>\n",
       "      <th>Article</th>\n",
       "      <th>Cleaned_Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>There are plenty of things that can impede wom...</td>\n",
       "      <td>plenty things impede women’s road career succe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Louisiana, New York, San Diego, New York.\\nNo ...</td>\n",
       "      <td>louisiana new york san diego new york city pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Paul Manafort pleaded guilty to two counts of ...</td>\n",
       "      <td>paul manafort pleaded guilty two counts conspi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Dozens of lawmakers with significant oppositio...</td>\n",
       "      <td>dozens lawmakers significant opposition trump’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>An ex-pimp whose book How to Lead A Slave Love...</td>\n",
       "      <td>expimp whose book lead slave lover paradise in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fake                                            Article  \\\n",
       "0     1  There are plenty of things that can impede wom...   \n",
       "1     1  Louisiana, New York, San Diego, New York.\\nNo ...   \n",
       "2     1  Paul Manafort pleaded guilty to two counts of ...   \n",
       "3     1  Dozens of lawmakers with significant oppositio...   \n",
       "4     1  An ex-pimp whose book How to Lead A Slave Love...   \n",
       "\n",
       "                                     Cleaned_Article  \n",
       "0  plenty things impede women’s road career succe...  \n",
       "1  louisiana new york san diego new york city pro...  \n",
       "2  paul manafort pleaded guilty two counts conspi...  \n",
       "3  dozens lawmakers significant opposition trump’...  \n",
       "4  expimp whose book lead slave lover paradise in...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'The shape of the filtered data frame is: {filtered_df.shape}')\n",
    "print (f\"Number of real articles {filtered_df.shape[0] - sum(filtered_df['Fake'])}\")\n",
    "print (f\"Number of AI generated articles {sum(filtered_df['Fake'])}\")\n",
    "filtered_df.iloc[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "bag_words = count_vectorizer.fit_transform(filtered_df['Cleaned_Article'])\n",
    "word_counts = pd.DataFrame({'word': vectorizer.get_feature_names_out(), 'count': bag_words.toarray().sum(axis=0)})\n",
    "word_counts.sort_values(by='count', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    104567.000000\n",
       "mean         29.982126\n",
       "std         255.024821\n",
       "min           1.000000\n",
       "25%           1.000000\n",
       "50%           2.000000\n",
       "75%           7.000000\n",
       "max       30584.000000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[\"count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word needs to appear at least 30 times (the mean word count for the vocabulary)\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of tfidf vocabulary is 0.09 of the overall vocabulary.\n",
      "['10' '100' '1000' ... 'zone' 'zones' 'zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "fake_tfidf = tfidf_vectorizer.fit_transform(filtered_df[\"Cleaned_Article\"])\n",
    "tfidf_vocabulary = tfidf_vectorizer.get_feature_names_out()\n",
    "count_vocabulary = count_vectorizer.get_feature_names_out(word_counts)\n",
    "size_tfidf = tfidf_vocabulary.shape[0]\n",
    "size_vocab = count_vocabulary.shape[0]\n",
    "size_compare = np.round(size_tfidf / size_vocab, 2)\n",
    "print (\"Size of tfidf vocabulary is \" + str(size_compare) +\n",
    "      \" of the overall vocabulary.\")\n",
    "print (tfidf_vocabulary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4allc6g3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
